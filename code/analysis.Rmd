---
title             : "Gender differences in infant mental rotation"
shorttitle        : "Infant mental rotation"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2,*"
    corresponding : "yes"
    address       : "Stephanstraße 1a, 04103 Leipzig"
    email         : "enge@cbs.mpg.de"
  - name          : "Shreya Kapoor"
    affiliation   : "1,*"
  - name          : "Anne-Sophie Kieslinger"
    affiliation   : "1,*"
  - name          : "Michael A. Skeide"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Research Group Learning in Early Childhood, Max Planck Institute for Human Cognitive and Brain Sciences"
  - id            : "2"
    institution   : "Department of Psychology, Humboldt-Universität zu Berlin"
  - id            : "*"
    institution   : "These authors contributed equally"

authornote: |
  \addORCIDlink{Alexander Enge}{0000-0003-0100-2297}

  \addORCIDlink{Shreya Kapoor}{0000-0003-2619-8257}

  The data and code for this study are openly available at ???.
  We have no conflict of interest to disclose.
  There were no ethical concerns since we did not collect any new data.

abstract: |
  Mental rotation, the cognitive process of moving an object in mind to predict how it looks in a new orientation, is tightly coupled to intelligence, learning and educational achievement [@shepard1971; @hegarty1999; @johnson2005]. On average, males solve mental rotation tasks slightly faster than females [@linn1985; @voyer1995; @voyer2011; @maeda2013]. When such behavioral differences emerge during development, however, remains poorly understood [@lauer2019a]. Here we analyzed effect sizes derived from 59 experiments conducted in 1,798 infants aged 3 to 16 months. We robustly found that male infants recognized novel rotated objects more reliably than female infants. The effect size of this average difference was small and did not change with age. These findings indicate that gender differences in mental rotation are already present in the first months of life.

keywords          : "mental rotation, spatial cognition, spatial ability, infants, children, gender, meta-analysis"
wordcount         : "X"

bibliography      : "references.bib"
csl               : "nature.csl"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa7"
classoption       : "man,donotrepeattitle"
fontsize          : "10pt"
output            : papaja::apa6_pdf
header-includes:
  - \geometry{a4paper}
  - \raggedbottom
  - \usepackage[all]{nowidow}
  - \usepackage{makecell}
  - \usepackage{setspace}
  - \renewcommand{\arraystretch}{1.0}
  - \captionsetup{font={stretch=1}, belowskip=15pt}
---

```{r, setup, include=FALSE}
# Load packages
library(papaja)
library(here)
library(readxl)
library(scales)
library(tidyverse)
library(magrittr)
library(metafor)
library(brms)
library(bayestestR)
library(tidybayes)
library(cowplot)

# Global chunk options
knitr::opts_chunk$set(
  include = FALSE,
  message = FALSE,
  fig.height = 10,
  fig.width = 7.5,
  out.width = "100%",
  warning = FALSE
)

# Directory paths
data_dir <- here("data")
figures_dir <- here("results/figures")
tables_dir <- here("results/tables")

# Download literature search spreadsheet from Google Drive
excel_url <- "https://docs.google.com/spreadsheets/d/1xnC0NlQYTXavXaeqQidhi7UKb1pCGAl9Ry3h3Uz3mi0"
excel_file <- here(data_dir, "literature_search.xlsx")
# googledrive::drive_auth(use_oob = TRUE)
# googledrive::drive_download(excel_url, excel_file, overwrite = TRUE)

# Read sheets
screening <- read_excel(excel_file, sheet = "screening")
included <- read_excel(excel_file, sheet = "included", na = "NA")
gender <- read_excel(excel_file, sheet = "gender_differences", na = "NA")

# Add columns to the table of included experiments
included %>%
  mutate(

    # Add unique experiment identifier
    experiment = str_c(year, article, group_long, sep = ", "),

    # Add difference between condition means
    mean_diff = case_when(
      !is.na(mean_diff) ~ mean_diff,
      TRUE ~ mean_novel - mean_familiar
    ),
    # Add d_z from paired t test of condition means (Rosenthal, 1991)
    d_z_t = t / sqrt(sample_size),
    # Add d_z from ANOVA F value via conversion to a t value
    d_z_f = sqrt(f) / sqrt(sample_size) * sign_f,
    # Add d_z from mean and standard deviation of the difference
    d_z_diff = mean_diff / sd_diff,
    # Add d_av from mean difference and standard devations
    # See http://dx.doi.org/10.20982/tqmp.14.4.p242
    sd_av = sqrt((sd_novel^2 + sd_familiar^2) / 2),
    d_av = mean_diff / sd_av,
    # Add d from one-sample t test of novelty preference scores
    d_nov_pref = (nov_pref - 0.5) / sd_nov_pref,

    # Choose one type of outcome variable for each experiment
    di = case_when(
      # 1. If d was reported directed
      !is.na(d) ~ d,
      # 2. If a paired sample t test was reported
      !is.na(d_z_t) ~ d_z_t,
      # 3. If ANOVA was reported
      !is.na(d_z_f) ~ d_z_f,
      # 4. If the difference between means and its SD were reported
      !is.na(d_z_diff) ~ d_z_diff,
      # 5. If the individual condition means and their SDs were reported
      !is.na(d_av) ~ d_av,
      # 6. If a novelty preference score and its SD were reported
      !is.na(d_nov_pref) ~ d_nov_pref
    ),
    # Keep track which type of outcome measure was chosen for each article
    di_type = case_when(
      !is.na(d) ~ "d",
      !is.na(d_z_t) ~ "d_z_t",
      !is.na(d_z_f) ~ "d_z_f",
      !is.na(d_z_diff) ~ "d_z_diff",
      !is.na(d_av) ~ "d_av",
      !is.na(d_nov_pref) ~ "d_nov_pref",
      TRUE ~ "none"
    ) %>%
      factor(levels = c(
        "d", "d_z_t", "d_z_f", "d_z_diff", "d_av", "d_nov_pref", "none"
      )),

    # Apply small sample correction using Hedges' exact method
    # See http://dx.doi.org/10.20982/tqmp.14.4.p242
    dfi = 2 * (sample_size - 1),
    ji = exp(lgamma(dfi / 2) - log(sqrt(dfi / 2)) - lgamma((dfi - 1) / 2)),
    gi = di * ji,

    # Recode gender as a categorical (factor) variable for meta-regression
    gender = case_when(
      female_percent == 1.0 ~ "Female",
      female_percent == 0.0 ~ "Male",
      TRUE ~ "Mixed"
    ) %>% factor(levels = c("Mixed", "Female", "Male")),

    # Recode mean sample age in years (centered) for meta-regression
    age = age_mean / 365.25,
    age_c = age - mean(age, na.rm = TRUE),
    age_sd = age_sd / 365.25,

    # Recode task type as a categorical (factor) variale for meta-regression
    task = factor(task, levels = c("Habituation", "VoE")),

    # Combine gender and task into one column for plotting
    gender_task = factor(
      str_c(gender, task, sep = ", "),
      levels = c(
        "Female, Habituation",
        "Male, Habituation",
        "Mixed, Habituation",
        "Mixed, VoE"
      )
    )
  ) %>%
  # Order rows by experiment ID
  arrange(experiment) -> dat_all

# Overview of the different effect sizes
dat_all %>%
  select(
    article,
    group,
    gender_split,
    gi,
    di,
    di_type,
    d,
    d_z_t,
    d_z_f,
    d_z_diff,
    d_av,
    d_nov_pref
  ) %>%
  print(n = Inf)

# Compute standard error of Cohen's d based on assumed correlation
# See Hedges' formula on p. 253 in http://dx.doi.org/10.20982/tqmp.14.4.p242
# This will need a sensitivity analysis regarding the values of `r_assumed`
r_assumed <- 0.5
dat_all %>%
  mutate(
    ni = sample_size,
    vi = (dfi / (dfi - 2)) * ((2 * (1 - r_assumed)) / ni) *
      (1 + gi^2 * (ni / (2 * (1 - r_assumed)))) - (gi^2 / ji^2),
    sei = sqrt(vi)
  ) %>%
  filter(!is.na(gi)) -> dat_all_r

# Show number of experiments per gender grouping strategy
table(dat_all_r$gender_split)

# Extract split or mixed experiments only
dat_split <- filter(dat_all_r, gender_split %in% c("split", "split_only"))
dat_mixed <- filter(dat_all_r, gender_split %in% c("mixed", "mixed_only"))

# Combine both strategies, preferring split experiments
dat_both_split <- filter(
  dat_all_r, gender_split %in% c("split", "mixed_only", "split_only")
)

# Combine both strategies, preferring mixed experiments
dat_both_mixed <- filter(
  dat_all_r, gender_split %in% c("mixed", "mixed_only", "split_only")
)

# We go ahead with the last solution as to use the maximum amount of information
# while not biasing the gender results
dat <- dat_both_mixed

# Helper functions to re-format age from days to months + days
format_days_months <- function(days, long = FALSE) {
  m_str <- ifelse(long, " months ", "m ")
  d_str <- ifelse(long, " days", "d")
  days_per_month <- 30.417
  months <- days %/% days_per_month
  days_left <- as.integer(days - months * days_per_month)
  str <- paste0(as.character(months), m_str, as.character(days_left), d_str)
  str[is.na(days)] <- NA
  return(str)
}

# Helper function for printing percentages
print_perc <- function(x) scales::percent(x, accuracy = 0.1)

# Extract some descriptive statistics for usage in the main text
descs <- list(
  n_experiments = nrow(dat),
  n_infants = sum(dat$sample_size),
  percent_female = mean(dat$female_percent, na.rm = TRUE),
  min_age_months = as.integer(min(dat$age_min, na.rm = TRUE) / 30.417),
  mean_age_weighted = format_days_months(
    sum(dat$age_mean * dat$ni) / (sum(dat$ni)),
    long = TRUE
  ),
  n_experiments_habituation = nrow(filter(dat, task == "Habituation")),
  n_experiments_voe = nrow(filter(dat, task == "VoE"))
)
```

# Introduction

The cognitive ability to move visual object representations in mind for recognition across different orientations, known as mental rotation, emerges in the first three months of life [@moore2020; @johnson2020]. Mental rotation is a key component of intelligence and a powerful predictor of learning outcome and educational achievement [@shepard1971; @hegarty1999; @johnson2005].

Previous meta-analyses revealed that males solve mental rotation tasks slightly faster than females on average [@linn1985; @voyer1995; @voyer2011; @maeda2013]. Effect sizes of this difference, however, are heterogeneous and often only medium in size (mean weighted $g = 0.37\text{--}0.73$). Interestingly, a recent meta-analysis [@lauer2019a] in 3-17-year-old children and adolescents suggests even only a small-to-medium difference (mean weighted $g = 0.39$). Whether gender differences in mental rotation behavior already emerge during infancy remains unknown.

In the present study, we meta-analyzed `r descs$n_experiments` effect sizes derived from looking times in mental rotation tasks conducted by `r formatC(descs$n_infants, big.mark = ",")` infants (`r print_perc(descs$percent_female)` female) aged `r descs$min_age_months` to `r descs$max_age_months` months (mean age of `r descs$mean_age_weighted`; Supplementary Table 1). All tasks were embedded either into habituation experiments (`r descs$n_experiments_habituation`) or violation of expectation experiments (`r descs$n_experiments_voe`) [@fantz1964; @baillargeon1985]. These experiments comprised real world stimuli (e.g., toy objects [@antrilli2016]), three-dimensional digital stimuli (e.g., cube figures [@moore2008]), or two-dimensional digital stimuli (e.g., digits [@quinn2008]). In habituation experiments, infants repeatedly saw an object until their looking times declined before they were presented with a mirror image of the object. Longer looking times at the mirror image were taken as evidence that an infant still recognized the familiar object after rotation through the new orientation. In violation of expectation experiments, infants were habituated to an object that was revolving repeatedly through a certain angle. Then, either the familiar object or an unseen object was shown while they were revolving through a certain angle. Subsequently, the object disappeared behind an occluder. Finally, the occluder was removed and either the object or its mirror object were shown. Larger differences in looking times for the familiar object versus the unseen mirror image (at the new angle) were taken as evidence that an infant still recognized the familiar object after rotation through the new angle. In contrast, looking times for both objects were expected to be similar if an infant did not recognize the familiar object from the new angle.

Following previous work in older populations, we hypothesized that looking times in mental rotation experiments are on average significantly longer in male compared to female infants. Effect sizes were assumed to be small.

# Results

## Mental rotation performance

```{r, meta_analysis}
# Specify priors
prior_c <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("cauchy(0, 0.3)", class = "sd")
)

# Run prior predictive simulation
n_iter <- 4000
n_warmup <- 1000
n_chains <- 4
res_prior <- brm(
  gi | se(sei) ~ 0 + Intercept + (1 | article / experiment),
  data = dat,
  prior = prior_c,
  sample_prior = "only",
  save_pars = save_pars(all = TRUE),
  chains = n_chains,
  iter = n_iter,
  warmup = n_warmup,
  cores = n_chains,
  control = list(adapt_delta = 0.99),
  file = here("results", "models", "res_prior")
)
summary(res_prior)
# plot(res_prior)

# Run Bayesian multilevel model
res_brm <- update(
  res_prior,
  sample_prior = FALSE,
  file = here("results", "models", "res_brm")
)
summary(res_brm)
# plot(res_brm)

# Get posterior probability mass > 0 for the meta-analytic effect
(hyp_brm <- hypothesis(res_brm, "Intercept > 0")$hypothesis)

# # Summarise fixed effects (incl. Bayes factors)
# describe_posterior(
#   res_brm,
#   centrality = "mean", ci = 0.95, ci_method = "ETI",
#   test = c("p_direction", "rope", "bayesfactor"),
#   rope_range = c(-0.1, 0.1)
# )

# Extract posterior draws for the meta-analytic effects
spread_draws(res_brm, `b_.*`, `sd_.*`, regex = TRUE, ndraws = NULL) %>%
  # Compute variances and ICC
  mutate(
    intercept = b_Intercept,
    sigma_article = sd_article__Intercept,
    sigma_experiment = `sd_article:experiment__Intercept`,
    sigma2_article = sigma_article^2,
    sigma2_experiment = sigma_experiment^2,
    sigma2_total = sigma2_article + sigma2_experiment,
    icc = sigma2_article / sigma2_total,
    .keep = "unused"
  ) -> draws_brm

# Summarize as mean and 95% credible intervals
(summ_brm <- as.list(mean_qi(draws_brm)))
```

```{r, fig1, include=TRUE, fig.height=14, fig.width=12, fig.cap="(ref:fig1-caption)"}
# Helper functions for printing numbers with exactly 2 decimal points
print_num <- function(x, digits = 2) {
  x <- format(round(x, digits), trim = TRUE, nsmall = digits)
  x[x == "NA"] <- NA
  return(x)
}

# Helper function for confidence intervals (optionally adding the mean)
print_ci <- function(lb, ub, mean = NULL) {
  mean_str <- ifelse(mean, paste0(print_num(mean), " "))
  paste0(mean_str, "[", print_num(lb), ", ", print_num(ub), "]")
}

# Get posterior draws for the effect in each experiment
epred_draws_brm <- epred_draws(res_brm, dat, ndraws = NULL) %>%
  mutate(experiment_f = factor(experiment))

# Create forest plot
dat %>%
  # Make sure the plot will be ordered alphabetically by experiment IDs
  arrange(experiment) %>%
  mutate(
    experiment_f = fct_rev(fct_expand(factor(experiment), "model")),
    # Show article labels only for the first experiment per article
    article = if_else(article == lag(article, default = ""), "", article),
    # Compute frequentist confidence intervals
    ci_lb = gi - qnorm(0.05 / 2, lower.tail = FALSE) * sqrt(vi),
    ci_ub = gi + qnorm(0.05 / 2, lower.tail = FALSE) * sqrt(vi),
    ci_print = print_ci(ci_lb, ci_ub, mean = gi)
  ) %>%
  # Prepare plotting canvas
  ggplot(aes(x = gi, y = experiment_f)) +
  geom_vline(xintercept = seq(-3, 3, 0.5), color = "grey90") +
  # Add article and group labels as text on the left
  geom_text(aes(x = -9.9, label = article), hjust = 0) +
  geom_text(aes(x = -7.1, label = group_long), hjust = 0) +
  # Add experiment-specific effect sizes and CIs as text on the right
  geom_text(aes(x = 3.7, label = print_num(gi)), hjust = 1) +
  geom_text(aes(x = 4.4, label = print_num(ci_lb)), hjust = 1) +
  geom_text(aes(x = 5.1, label = print_num(ci_ub)), hjust = 1) +
  # Add Bayesian highest posterior density intervals for each experiment
  stat_interval(
    aes(x = .epred),
    data = epred_draws_brm,
    alpha = .8,
    point_interval = "mean_qi",
    .width = c(0.5, 0.95),
  ) +
  scale_color_grey(
    start = 0.85, end = 0.65,
    labels = as_mapper(~ scales::percent(as.numeric(.x)))
  ) +
  # Add experiment-specific effect sizes and CIs as dots with error bars
  geom_linerange(aes(xmin = ci_lb, xmax = ci_ub), size = 0.35) +
  geom_point(aes(size = ni), shape = 22, fill = "white") + # Or (1 / vi)?
  # Add posterior distribution for the meta-analytic effect
  stat_halfeye(
    aes(x = intercept, y = -1),
    draws_brm,
    point_interval = "mean_qi",
    .width = c(0.95)
  ) +
  annotate(
    "text",
    x = c(-9.9, 3.7, 4.4, 5.1),
    y = -0.5,
    label = c(
      "Three-level model",
      print_num(summ_brm$intercept),
      print_num(summ_brm$intercept.lower),
      print_num(summ_brm$intercept.upper)
    ),
    hjust = c(0, 1, 1, 1),
    fontface = "bold"
  ) +
  # Add headers for each of the for columns
  annotate(
    "rect",
    xmin = -Inf,
    xmax = Inf,
    ymin = descs$n_experiments + 0.5,
    ymax = Inf,
    fill = "white"
  ) +
  annotate(
    "text",
    x = c(-9.9, -7.1, 0, 3.7, 4.4, 5.1),
    y = descs$n_experiments + 1.5,
    label = c(
      "Article",
      "Experiment",
      "Effect size plot",
      "Effect size",
      "Lower",
      "Upper"
    ),
    hjust = c(0, 0, 0.5, 1, 1, 1),
    fontface = "bold"
  ) +
  # Styling
  coord_cartesian(
    ylim = c(-1.5, descs$n_experiments + 2), expand = FALSE, clip = "off"
  ) +
  scale_x_continuous(breaks = seq(-3, 3, 0.5)) +
  annotate("segment", x = -3.3, xend = 3.3, y = -1.5, yend = -1.5) +
  labs(
    x = expression("Hedges'" ~ italic("g")),
    size = "Sample size",
    color = "CrI level"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(family = "Helvetica", color = "black"),
    axis.text.y = element_blank(),
    axis.ticks.x = element_line(colour = "black"),
    axis.title.x = element_text(hjust = 0.67, margin = margin(b = -15)),
    axis.title.y = element_blank(),
    panel.grid = element_blank(),
    text = element_text(family = "Helvetica")
  )

# Save the plot
ggsave(here(figures_dir, "forest.pdf"), width = 12, height = 14)
```

(ref:fig1-caption) **Mental rotation performance.** A Bayesian three-level meta-analysis provided evidence for mental rotation ability in infants. White squares depict the effect sizes (Hedges’ $g$) for infants’ mental rotation performance in all individual experiments and black lines depict their 95% confidence intervals. Gray bars indicate the 95% and 50% Bayesian credible intervals (CrI). These intervals got shrunk towards the meta-analytic effect size because of partial pooling which regularized the impact of experiments with small sample sizes or unrealistically large effect sizes to prevent overfitting the model. The last line shows the meta-analytic effect size (black dot) together with its 95% CrI (black line) and its posterior distribution (gray curve).

For our first meta-analytic model effect sizes were quantified as the standardized mean difference in infants’ looking times for novel and familiar rotated objects. Using this effect size index we ran a Bayesian three-level random-effects model to test if there was evidence that infants did perform mental rotation. Across studies, infants indeed looked longer at novel rotated objects than at familiar rotated objects, with a standardized mean difference of $g = `r summ_brm$intercept`$, 95% credible interval (CrI) [`r summ_brm$intercept.lower`, `r summ_brm$intercept.upper`] (Fig. 1). The probability for this effect being greater than zero was `r print_perc(hyp_brm$Post.Prob)`. The heterogeneity of effect sizes was $\sigma_{\text{Experiment}}^2 = `r summ_brm$sigma2_experiment`$, 95% CrI [`r summ_brm$sigma2_experiment.lower`, `r summ_brm$sigma2_experiment.upper`], at the experiment level and $\sigma_{\text{Article}}^2 = `r summ_brm$sigma2_article`$, 95% CrI [`r summ_brm$sigma2_article.lower`, `r summ_brm$sigma2_article.upper`] at the article level. Therefore, approximately `r print_perc(1 - summ_brm$icc)` of the heterogeneity between effect sizes was attributable to differences between experiments *within* articles and `r print_perc(summ_brm$icc)` was attributable to differences *between* articles.

## Effects of gender, age, and task type

```{r, meta_regression}
# Set numerical contrasts for factor variables
contrasts(dat$gender) <- MASS::contr.sdif(3)
contrasts(dat$task) <- MASS::contr.sdif(2)

# Specify priors
prior_c <- c(
  set_prior("normal(0, 1)", class = "b", coef = "Intercept"),
  set_prior("normal(0, 0.5)", class = "b"),
  set_prior("cauchy(0, 0.3)", class = "sd")
)

# Run prior predictive simulation
res_prior_reg <- brm(
  gi | se(sei) ~ 0 + Intercept + gender * age_c + task +
    (1 | article / experiment),
  data = dat,
  prior = prior_c,
  sample_prior = "only",
  save_pars = save_pars(all = TRUE),
  chains = n_chains,
  iter = n_iter,
  warmup = n_warmup,
  cores = n_chains,
  control = list(adapt_delta = 0.99),
  file = here("results", "models", "res_prior_reg")
)
summary(res_prior_reg)
# plot(res_prior_reg)

# Run Bayesian multilevel model
res_brm_reg <- update(
  res_prior_reg,
  sample_prior = FALSE,
  file = here("results", "models", "res_brm_reg")
)
summary(res_brm_reg)
# plot(res_brm_reg)

# Get posterior probability mass > 0 for all regression weights
(hyps_brm_reg <- list(
  intercept = hypothesis(res_brm_reg, "Intercept > 0")$hypothesis,
  female_mixed = hypothesis(res_brm_reg, "gender2M1 < 0")$hypothesis,
  male_female = hypothesis(res_brm_reg, "gender3M2 > 0")$hypothesis,
  age = hypothesis(res_brm_reg, "age_c > 0")$hypothesis,
  voe_habituation = hypothesis(res_brm_reg, "task2M1 > 0")$hypothesis,
  female_mixed_age = hypothesis(res_brm_reg, "gender2M1:age_c > 0")$hypothesis,
  male_female_age = hypothesis(res_brm_reg, "gender3M2:age_c > 0")$hypothesis
))

# # Summarise fixed effects (incl. Bayes factors)
# describe_posterior(
#   res_brm_reg,
#   centrality = "mean", ci = 0.95, ci_method = "ETI",
#   test = c("p_direction", "rope", "bayesfactor"),
#   rope_range = c(-0.1, 0.1)
# )

# Extract posterior draws for the meta-analytic effects
spread_draws(res_brm_reg, `b_.*`, `sd_.*`, regex = TRUE, ndraws = NULL) %>%
  # Compute variances and ICC
  mutate(
    intercept = b_Intercept,
    female_mixed = b_gender2M1,
    male_female = b_gender3M2,
    age = b_age_c,
    voe_habituation = b_task2M1,
    female_mixed_age = `b_gender2M1:age_c`,
    male_female_age = `b_gender3M2:age_c`,
    sigma_article = sd_article__Intercept,
    sigma_experiment = `sd_article:experiment__Intercept`,
    sigma2_article = sd_article__Intercept^2,
    sigma2_experiment = `sd_article:experiment__Intercept`^2,
    sigma2_total = sigma2_article + sigma2_experiment,
    icc = sigma2_article / sigma2_total,
  ) -> draws_brm_reg

# Summarize as mean and 95% credible intervals
(summ_brm_reg <- as.list(mean_qi(draws_brm_reg)))
```

As a next step, we conducted a meta-regression analysis to test if the gender of the infants, their age, or the type of mental rotation task was related to mental rotation performance (Fig. 2).
Indeed, experiments with all-male samples revealed larger looking time differences than experiments with all-female samples, $b = `r summ_brm_reg$male_female`$, 95% CrI [`r summ_brm_reg$male_female.lower`, `r summ_brm_reg$male_female.upper`].
The probability for this effect being larger than zero was `r print_perc(hyps_brm_reg$male_female$Post.Prob)`.
We found no difference between mixed-gender and all-female samples, $b = `r -summ_brm_reg$female_mixed`$, 95% CrI [`r -summ_brm_reg$female_mixed.lower`, `r -summ_brm_reg$female_mixed.upper`] (`r print_perc(hyps_brm_reg$female_mixed$Post.Prob)` probability of an effect larger than zero).
Additionally, mean age was not related to mental rotation performance, with a change per year of $b = `r summ_brm_reg$age`$, 95% [`r summ_brm_reg$age.lower`, `r summ_brm_reg$age.upper`].
We also did not detect an interaction between gender and age ([females - mixed] × age: $b = `r summ_brm_reg$female_mixed_age`$, 95% CrI [`r summ_brm_reg$female_mixed_age.lower`, `r summ_brm_reg$female_mixed_age.upper`]; [males - females] × age: $b = `r summ_brm_reg$male_female_age`$, 95% CrI [`r summ_brm_reg$male_female_age.lower`, `r summ_brm_reg$male_female_age.upper`]).
Finally, there was weak evidence that violation of expectation tasks yielded larger effects than habituation tasks, $b = `r summ_brm_reg$voe_habituation`$, 95% [`r summ_brm_reg$voe_habituation.lower`, `r summ_brm_reg$voe_habituation.upper`].
The probability for this effect being greater than zero was `r print_perc(hyps_brm_reg$voe_habituation$Post.Prob)`.

```{r, fig2, include=TRUE, fig.height=5, fig.width=12, fig.cap="(ref:fig2-caption)"}
# Get draws from the posterior distribution
draws_reg <- epred_draws(
  res_brm_reg,
  dat,
  ndraws = NULL,
  re_formula = NA
)

# Use colors as proposed in https://arxiv.org/abs/2107.02270
habi_colors <- c(
  "Female, Habituation" = "#5790fc",
  "Male, Habituation" = "#f89c20",
  "Mixed, Habituation" = "#e42536"
)
voe_colors <- c("Mixed, VoE" = "#964a8b")

# Regression plot
dat %>%
  ggplot(aes(x = age * 12, y = gi)) +
  geom_hline(yintercept = seq(-1, 2, 0.5), color = "grey90") +
  annotate(
    "rect",
    xmin = 5.5, xmax = Inf, ymin = 1.25, ymax = Inf,
    color = NA, fill = "white"
  ) +
  # Regression lines and HPDIs - Habituation tasks
  stat_lineribbon(
    aes(y = .epred, color = gender_task),
    data = draws_reg,
    alpha = .4,
    point_interval = "mean_qi",
    .width = c(0.5, 0.95)
  ) +
  # Individual study effect sizes - Habituation tasks
  geom_point(aes(color = gender_task, size = ni), shape = 0) +
  scale_color_manual(
    values = habi_colors,
    breaks = names(habi_colors),
    labels = c("Females", "Males", "Mixed gender"),
    na.value = NA
  ) +
  guides(color = guide_legend(
    title = "Habituation tasks",
    override.aes = list(fill = NA),
    order = 1
  )) +
  # Regression lines and HPDIs - VoE tasks
  ggnewscale::new_scale_color() +
  stat_lineribbon(
    aes(y = .epred, color = gender_task),
    data = draws_reg,
    fill = NA,
    alpha = .4,
    point_interval = "mean_qi",
    .width = c(0.5, 0.95)
  ) +
  # Individual study effect sizes - VoE tasks
  geom_point(aes(color = gender_task, size = ni), shape = 0) +
  scale_color_manual(
    values = voe_colors,
    breaks = names(voe_colors),
    labels = c("Mixed gender"),
    na.value = NA
  ) +
  guides(color = guide_legend(title = "VoE tasks", order = 2)) +
  # Styling
  coord_cartesian(expand = FALSE) +
  scale_x_continuous(limits = c(2.9, 16.1), breaks = seq(3, 16, 1)) +
  scale_y_continuous(limits = c(-1.2, 2.2), breaks = seq(-1, 2, 0.5)) +
  scale_fill_grey(
    start = 0.85, end = 0.65,
    labels = as_mapper(~ scales::percent(as.numeric(.x)))
  ) +
  labs(
    x = "Age (months)",
    y = expression("Hedges'" ~ italic("g")),
    fill = "CrI level",
    size = "Sample size"
  ) +
  theme_classic() +
  theme(
    axis.text = element_text(family = "Helvetica", color = "black"),
    axis.ticks = element_line(color = "black"),
    legend.direction = "vertical",
    legend.key = element_blank(),
    legend.position = "none",
    panel.grid = element_blank(),
    text = element_text(family = "Helvetica", color = "black")
  ) -> plot_reg

# Extract the legend so that we can put it inside the plotting area
plot_reg_legend <- get_legend(plot_reg + theme(legend.position = "top"))

# Define new labels for the regression coefficients
coef_colnames <- c(
  "Intercept" = "b_Intercept",
  "Females - mixed" = "b_gender2M1",
  "Males - females" = "b_gender3M2",
  "Age (per year)" = "b_age_c",
  "Habituation - VoE" = "b_task2M1",
  "(Females - Mixed) × age" = "b_gender2M1:age_c",
  "(Males - Females) × age" = "b_gender3M2:age_c"
)

# Plot posterior distributions of the regression coefficients
draws_reg_coef <- tidy_draws(res_brm_reg)
draws_reg_coef %>%
  select(all_of(coef_colnames)) %>%
  gather() %>%
  mutate(coef = factor(key, levels = names(coef_colnames))) %>%
  ggplot(aes(x = value, y = fct_rev(coef))) +
  geom_vline(xintercept = seq(-1, 1, 0.5), color = "grey90") +
  stat_halfeye(point_interval = "mean_qi", .width = c(0.5, 0.95)) +
  coord_cartesian(xlim = c(-1.25, 1.25), clip = "off") +
  scale_x_continuous(breaks = seq(-1, 1, 0.5)) +
  labs(x = expression("Regression weight (" * Delta * italic("g") * ")")) +
  annotate(
    "text",
    label = "Fixed effects", x = -3.175, y = 7.86, hjust = 0,
    family = "Helvetica", fontface = "bold", color = "black"
  ) +
  theme_minimal() +
  theme(
    axis.line.x = element_line(colour = "black"),
    axis.text.x = element_text(family = "Helvetica", color = "black"),
    axis.text.y = element_text(
      family = "Helvetica", color = "black", size = 3.88 * .pt,
      hjust = 0, vjust = -0.1, margin = margin(l = 20)
    ),
    axis.ticks.x = element_line(color = "black"),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    panel.grid = element_blank(),
    text = element_text(family = "Helvetica", color = "black")
  ) -> plot_coef

# Combine plots
plot_grid(
  plot_reg, plot_coef,
  nrow = 1, rel_widths = c(3, 2), labels = "auto",
  label_size = 11, label_fontfamily = "Helvetica",
  label_x = 0.008, label_y = 0.99
) +
  draw_plot(plot_reg_legend, x = 0.385, y = 0.865, hjust = 0.5, vjust = 0.5)

# Save plot
ggsave(here(figures_dir, "regression_coef.pdf"), width = 12, height = 5)
```

(ref:fig2-caption) **Effects of gender, age, and task.** **a,** Squares show the effect size (Hedges’ g) for infants’ mental rotation performance in each of the 59 individual experiments. Squares are color-coded according to the type of habituation task and the gender of the infants (blue = habituation task, all-female sample, yellow = habituation task, all-male sample, red = habituation task, mixed-gender sample, purple = violation of expectation [VoE] task, mixed-gender sample). Lines indicate the best-fit regression estimates according to a Bayesian three-level meta-regression model and gray ribbons indicate their corresponding 95% and 50% credible interval (CrI). **b,** Fixed effect estimates obtained from the Bayesian three-level meta-regression model are depicted as black dots together with their 95% Crl (thin black lines) and 50% Crl (thick black lines). Gray curves indicate the posterior distribution for each effect.

```{r, gender_differences}
# Extract relevant effect sizes
gender %>%
  # Remove redundant samples
  filter(!redundant) %>%
  # Add columns
  mutate(

    # Add unique experiment identifier
    experiment = str_c(year, article, group_long, sep = ", "),

    # Re-code non-significant f values as an effect of 0
    f = as.numeric(f),
    f_assumed = ifelse(is.nan(f), 0, f),

    # Add d from two-samples t test (Lakens, 2013)
    d_t = t * sqrt(1 / female_n + 1 / male_n),

    # Add d from ANOVA F value via conversion to a t value
    d_f = sqrt(f_assumed) * sqrt(1 / female_n + 1 / male_n),

    # Add d from mean difference and pooled standard deviation
    mean_diff = mean_diff_males_mean - mean_diff_females_mean,
    sd_diff_pooled_numerator = (male_n - 1) * (mean_diff_males_sd^2) +
      (female_n - 1) * (mean_diff_females_sd^2),
    df = male_n + female_n - 2,
    sd_diff_pooled = sqrt(sd_diff_pooled_numerator / df),
    d_diff = mean_diff / sd_diff_pooled,

    # Add d from one-sample t test of novelty preference scores
    mean_nov_pref = novelty_pref_males_mean - novelty_pref_females_mean,
    sd_nov_pref_pooled_numerator = (male_n - 1) * (novelty_pref_males_sd^2) +
      (female_n) * (novelty_pref_females_sd^2),
    sd_nov_pref_pooled = sqrt(sd_nov_pref_pooled_numerator / df),
    d_nov_pref = mean_nov_pref / sd_nov_pref_pooled,

    # Choose one type of outcome variable for each experiment
    di = case_when(
      # 1. If d was reported directed
      !is.na(d) ~ d,
      # 2. If a t test was reported
      !is.na(d_t) ~ d_t,
      # 3. If ANOVA was reported
      !is.na(d_f) ~ d_f,
      # 4. If the difference between means and their SDs were reported
      !is.na(d_diff) ~ d_diff,
      # 5. If novelty preference scores and their SDs were reported
      !is.na(d_nov_pref) ~ d_nov_pref
    ),
    # Keep track which type of outcome measure was chosen for each article
    di_type = case_when(
      !is.na(d) ~ "d",
      !is.na(d_t) ~ "d_t",
      !is.na(d_f) ~ "d_f",
      !is.na(d_diff) ~ "d_diff",
      !is.na(d_nov_pref) ~ "d_nov_pref"
    ) %>%
      factor(levels = c("d", "d_t", "d_f", "d_diff", "d_nov_pref")),

    # Apply small sample correction using Hedges' exact method
    # See http://dx.doi.org/10.20982/tqmp.14.4.p242
    j = exp(lgamma(df / 2) - log(sqrt(df / 2)) - lgamma((df - 1) / 2)),
    gi = di * j,

    # Compute standard error of Cohen's d, using harmonic mean of sample sizes
    ni = female_n + male_n,
    nhi = (female_n * male_n) / ni,
    vi = (df * 2 * (1 + (gi^2 * nhi * 0.5))) / ((df - 2) * nhi) - (gi^2 / j^2),
    sei = sqrt(vi)
  ) %>%
  # Remove experiments that didn't provide an effect size
  filter(!is.na(gi)) -> dat_gender

# Overview of the different effect sizes
dat_gender %>%
  select(
    article,
    group_long,
    gi,
    di,
    di_type,
    d,
    d_t,
    d_f,
    d_diff,
    sei
  ) %>%
  print(n = Inf)

# Specify priors
prior_c <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("cauchy(0, 0.3)", class = "sd")
)

# Run prior predictive simulation
res_prior_gender <- brm(
  gi | se(sei) ~ 0 + Intercept + (1 | article / experiment),
  data = dat_gender,
  prior = prior_c,
  sample_prior = "only",
  save_pars = save_pars(all = TRUE),
  chains = n_chains,
  iter = n_iter,
  warmup = n_warmup,
  cores = n_chains,
  control = list(adapt_delta = 0.99),
  file = here("results", "models", "res_prior_gender")
)
summary(res_prior_gender)
# plot(res_prior_gender)

# Run Bayesian multilevel model
res_brm_gender <- update(
  res_prior_gender,
  sample_prior = FALSE,
  file = here("results", "models", "res_brm_gender")
)
summary(res_brm_gender)
# plot(res_brm_gender)

# Get posterior probability mass > 0 for the meta-analytic effect
(hyp_brm_gender <- hypothesis(res_brm_gender, "Intercept > 0")$hypothesis)

# # Summarise fixed effects (incl. Bayes factors)
# describe_posterior(
#   res_brm_gender,
#   centrality = "mean", ci = 0.95, ci_method = "ETI",
#   test = c("p_direction", "rope", "bayesfactor"),
#   rope_range = c(-0.1, 0.1)
# )

# Extract posterior draws for the meta-analytic effects
spread_draws(res_brm_gender, `b_.*`, `sd_.*`, regex = TRUE, ndraws = NULL) %>%
  # Compute variances and ICC
  mutate(
    intercept = b_Intercept,
    sigma_article = sd_article__Intercept,
    sigma_experiment = `sd_article:experiment__Intercept`,
    sigma2_article = sigma_article^2,
    sigma2_experiment = sigma_experiment^2,
    sigma2_total = sigma2_article + sigma2_experiment,
    icc = sigma2_article / sigma2_total,
    .keep = "unused"
  ) -> draws_brm_gender

# Summarize as mean and 95% credible intervals
(summ_brm_gender <- as.list(mean_qi(draws_brm_gender)))
```

To confirm the gender difference between males and females we set up another Bayesian meta-analysis, this time focusing on the looking-time contrasts between all-male and all-female groups reported *within* each experiment by the original authors.
Our additional analysis revealed a meta-analytic effect size of $g = `r summ_brm_gender$intercept`$, 95% Crl [`r summ_brm_gender$intercept.lower`, `r summ_brm_gender$intercept.upper`] (Supplementary Fig. 1) and a probability of this effect being greater than zero of `r print_perc(hyp_brm_gender$Post.Prob)`.
The heterogeneity of effect sizes was $\sigma_{\text{Experiment}}^2 = `r summ_brm_gender$sigma2_experiment`$, 95% CrI [`r summ_brm_gender$sigma2_experiment.lower`, `r summ_brm_gender$sigma2_experiment.upper`], at the experiment level and $\sigma_{\text{Article}}^2 = `r summ_brm_gender$sigma2_article`$, 95% CrI [`r summ_brm_gender$sigma2_article.lower`, `r summ_brm_gender$sigma2_article.upper`] at the article level. This means that `r print_perc(summ_brm_gender$icc)` of the total variance can be attributed to differences *between* articles and `r print_perc(1 - summ_brm_gender$icc)` to differences between experiments *within* articles.

## Publication bias and certainty assessment

```{r, publication_bias, echo=TRUE}
# Classical Egger regression test
(egger <- regtest(x = dat$gi, sei = dat$sei, model = "lm", predictor = "sei"))
(egger_ci <- confint(egger$fit)["Xsei", ])
```

By visual inspection of a funnel plot of standard errors and effect sizes we detected a slight asymmetry indicating a small publication bias (Fig. 3). Publication bias was also suggested by the results of an Egger regression test indicating that the slope of a weighted linear regression of the effect sizes on the standard errors is significantly different from zero in a two-sided $t$-test, $b = `r coef(egger$fit)["Xsei"]`$, $t(`r egger$dfs`) = `r egger$zval`$, $p = `r print_p(egger$pval)`$, 95% confidence interval (CI) [`r egger_ci["2.5 %"]`, `r egger_ci["97.5 %"]`]. Nevertheless, a jackknife leave-one-out analysis confirmed that the current results are robust to the effects of individual studies (Supplementary Table 3).

```{r, fig3, include=TRUE, fig.height=3.5, fig.width=5.56, fig.cap="(ref:fig3-caption)", out.width="50%"}
# Compute funnel
funnel_z_crit <- stats::qnorm(0.975)
funnel_max_se <- 0.5 # A bit larger than the largest observed `sei` in `dat`
funnel_tau2 <- 0 # We don't take heterogeneity into account, only the SE
funnel <- data.frame(
  x = c(
    summ_brm$intercept - funnel_z_crit * sqrt(funnel_max_se^2 + funnel_tau2),
    summ_brm$intercept - funnel_z_crit * sqrt(funnel_tau2),
    summ_brm$intercept + funnel_z_crit * sqrt(funnel_tau2),
    summ_brm$intercept + funnel_z_crit * sqrt(funnel_max_se^2 + funnel_tau2)
  ),
  y = c(funnel_max_se, 0, 0, funnel_max_se)
)

# Create funnel plot
dat %>%
  ggplot(aes(x = gi, y = sei)) +
  geom_vline(xintercept = seq(-2, 2, 0.5), color = "grey90") +
  # Meta-analytic effect size
  geom_vline(xintercept = summ_brm$intercept, color = "black") +
  # Experiment-specific effect sizes
  geom_point(shape = 0) +
  # Funnel
  geom_path(data = funnel, aes(x = x, y = y), linetype = "dashed") +
  # Styling
  scale_x_continuous(breaks = seq(-2, 2, 0.5)) +
  scale_y_reverse() +
  coord_cartesian(
    xlim = c(-2.5, 2.5),
    ylim = c(funnel_max_se, 0),
    expand = FALSE
  ) +
  labs(
    x = expression("Hedges'" ~ italic("g")),
    y = "Standard error",
  ) +
  theme_classic() +
  theme(
    axis.line = element_line(colour = "black"),
    axis.text = element_text(family = "Helvetica", color = "black"),
    axis.ticks = element_line(color = "black"),
    text = element_text(family = "Helvetica", color = "black")
  )

# Save the plot
ggsave(here(figures_dir, "funnel.pdf"), width = 5.56, height = 3.5)
```

(ref:fig3-caption) **Evaluation of publication bias.** The funnel plot shows the standard errors and the effect sizes for each of the individual studies. The funnel contours (dashed lines) show a 95%-pseudo-confidence interval of the standard error around the meta-analytic effect size (vertical solid line). A slight asymmetry induced by the underrepresentation of studies with high standard error and small effect size suggest a small publication bias. 

# Discussion

We analyzed looking times during mental rotation in 1,798 infants ranging from 3 to 16 months of age.
To this end, we scrutinized the robustness of 59 experimental effect sizes.
We found that male infants looked on average slightly longer at novel rotated objects compared to female infants.
This effect was small and unrelated to age in the current range.

## Effect size estimates

We interpret the meta-regression-based estimate of the gender difference (b = 0.48, b = $Delta$g ) as an upper bound of the true effect size since this estimate is based on experiments that reported only separate effect sizes for males and females.
The gender effect of these experiments can be considered as positively biased when assuming that authors who observe a statistically significant gender difference more likely report separate effect sizes for males and females.
In contrast, the meta-analytic estimate derived from the observed gender differences within each experiment (g = .20) can be viewed as a lower bound of the true effect size.
This view is plausible because the unknown effect sizes of experiments without significant gender differences were set to zero although non-zero differences some of these experiments will have found a non-zero gender difference but the size of this effect was not reported because of a lack of power to render it statistically significant.

## Gender differences

The gender differences observed here remain to be explained by interacting genetic and environmental factors that are largely unknown.
To the best of our knowledge, there are currently no genetic association or gene-environment interaction studies with a focus on mental rotation.
Nevertheless, it is documented that genetic contributions to behavioral variance in mental rotation are substantially smaller than unique non-shared environmental contributions both in male and female adults (Shakeshaft et al. 2016).
Whether this observation also applies to infants remains to be explored.

One recent study on 5-6-month-old female infants provided preliminary evidence for possible social-environmental effects related to parental attitudes towards gender which might partly explain the results of our present work (Constantinescu et al. 2018).
As far as we know, potentially mediating and moderating factors that could already be operational in infancy, however, are not yet empirically established.
In a similar vein, while mental rotation training has small-to-medium post-test effects in children, it is unclear whether it can remove gender differences and be adapted to infants Uttal et al. (2013).

Sex hormone concentration in male infants, especially postnatal testosterone in the first six months of life, could also contribute to gender differences in mental rotation performance (Toivainen et al. 2018, Constantinescu et al. 2018, Erdmann et al. 2019).
Possible biological developmental pathways, however, bridging the gap from hormonal to behavioral differences are currently far from understood.

## Additional factors

A number of additional factors have been associated with individual differences in infant mental rotation performance.
For example, mental rotation is related to previous relevant experience with the particular objects used in the specific task (Möhring and Frick 2013; Schwarzer et al. 2013b; Slone et al. 2018).
This relation also applies to previous experience with manually rotating toys (Schwarzer et al. 2013b).
While these preliminary results require replication they are in line with the longstanding notion that prior knowledge is the strongest predictor of learning outcomes in a range of cognitive domains (e.g. Ausubel 1968; Bradley & Bryant 1983; Halberda et al. 2008).
Furthermore, there is yet to be confirmed preliminary evidence for possible links between mental rotation performance and several sensory motor skills including fine and gross motor skills, oculomotor control and crawling skills (Schwarzer et al. 2013a; Schwarzer et al. 2013b).

# Conclusion

The present study robustly revealed that male infants look slightly but significantly longer at novel rotated objects than female infants.
Thus, on average, males outperform females in mental rotation already in the first months of life.

# Methods

## Protocol

We used established PRISMA 2020 (Preferred Reporting Items for Systematic reviews and Meta-Analyses; Page et al., 2021) guidelines to conduct this meta-analysis.
Fig.
4 presents the PRISMA flowchart and Supplementary Table 2 contains the PRISMA checklist.

```{r, fig4, include=TRUE, fig.width=6, fig.cap="(ref:fig4-caption)", out.width="50%"}
# Include the flowchart (created with MS PowerPoint)
knitr::include_graphics(here("results/figures/flowchart.pdf"))
```

(ref:fig4-caption) Literature search and selection process. We searched four online databases as well as review articles and reference sections to identify articles that reported mental rotation experiments in infants. Experiments from these articles were included in the meta-analysis if they fulfilled six pre-specified inclusion criteria. Redundant articles that reported the same experiment(s) as another article were excluded. aAdditional reasons for excluding articles were an experimental paradigm that differed substantially compared to all other articles (1 article) and a sample of infants that was substantially older compared to all other articles (1 article).

## Eligibility criteria

Articles needed to fulfill six criteria for being included in the meta-analysis: (1) The article was written English or German; (2) The article includes results from a group study with human participants (excluding review articles, meta-analyses, case studies, and animal studies); (3) These participants include at least one group of infants (mean age between 0.0 and 3.0 years); (4) Infants had no preterm status or clinical diagnosis; (5) Infants performed a mental rotation task; (6) The article contains quantitative scores that could be converted into a standardized mean difference (see below).
We explicitly included works that were not peer reviewed (e.g., dissertations and preprints) to reduce the impact of publication bias.

## Information sources and search strategy

We entered the search terms ("mental rotation" OR "mental transformation" OR "spatial rotation" OR "spatial transformation" OR "spatial ability" OR "spatial skills") AND ("infant" OR "infants" OR "infanthood" OR "toddler" OR "toddlers" OR "toddlerhood" OR "child" OR "children" OR "childhood" OR "month" OR "months") into four online databases (APA PsycINFO, PubMed/MEDLINE, Scopus, ProQuest Dissertations & Theses Global).
All database queries were completed on December 6, 2021.
We configured the databases to check for article titles, abstracts, and keywords while applying no other filters or limits.
This yielded 2,616 articles in total, 1,954 of which remained after removing duplicate records (Figure 1).
We further identified 76 articles by screening the reference sections of previous reviews and meta-analyses on mental rotation and related skills (Frick et al., 2014; Johnson & Moore, 2020; Kubicek & Schwarzer, 2018; Lauer et al., 2019; Linn & Petersen, 1985; Moore & Johnson, 2020; Uttal et al., 2013; Voyer et al., 1995; Yang et al., 2020).
Of these, 34 had not been covered by the database search.
We also identified 94 articles by screening the reference sections of all articles that had been included after the first pass of the selection process.
Of these, 49 had not been covered by the database search.
We thus screened 2,037 unique articles in total.

## Selection process

Two independent raters read the abstract and, if necessary, relevant sections of the full text to check if an article fulfilled the inclusion criteria.
Interrater agreement for the binary decision to include versus exclude an article was 98.5% ($Kappa$w [Cohen's weighted kappa] = 0.67, 95% CI [0.55, 0.78]).
Interrater agreement for the specific eligibility criteria were 88.2% ($Kappa$w = 0.72, 95% CI [0.40, 1.00]).
Cases where the two ratings diverged were resolved via discussion among all raters until a consensus was reached.
One article (Mash et al., 2007) was excluded because the authors used a unique mental rotation paradigm that was not comparable to that used in the other articles.
One article (Pedrett et al., 2020) was excluded because the average age of the infants (30.7 months) was almost twice as high as that of the next article (15.8 months), z = ???
compared to all articles.
We therefore decided to narrow our analysis from the first 3 years of life to the first 16 months of life.
This procedure led to a total of 23 articles being included in the meta-analysis (Fig. 4).

Many of these articles consisted of multiple experiments, e.g., using different variations of the mental rotation task or different subpopulations of infants.
We included all of these experiments in the meta-analysis and accounted for the dependencies between them by means of multilevel modeling with by-article random effects (see below).
However, we excluded experiments if there was insufficient information to compute a standardized effect size (see below).
Whenever an article reported separate effect sizes for boys and girls---or other subgroups like crawling and non-crawling infants)---but also an effect size combining these groups, we only included the combined effect size.
We dropped effect sizes that were clearly based on the same data but reported in different articles.
This procedure led to a total of 59 experiments being included in the meta-analysis (Supplementary Table 1).

## Data collection process and items

Outcome measures and other relevant variables were extracted from each article by one of three raters and verified by a second rater.
Outcome measures were any summary statistic (Table 1) that could be used to determine the standardized mean difference between novel/unexpected rotation events and familiar/expected rotation events (see Introduction).
Other extracted variables included, if available, the sample size, the number of girls and boys, the mean age and its standard deviation, the minimum and maximum age, the type of mental rotation task (habitation or violation of expectation), the modality of stimulus presentation (real objects or objects on a computer screen), and the dimensionality of the stimuli (2D or 3D; Supplementary Table 1).

```{r, tab1, include=TRUE}
print("Nice table goes here")
```

No study investigators were contacted for obtaining or confirming additional data and no automated tools were used in the data collection process.

## Effect size measures

One outcome measure per experiment was converted into a standardized mean difference with small sample correction (Hedges' g) using the formulas provided in Table 1.
The sampling variance Var_g of the outcome measure g for each experiment was computed using the formula provided by Goulet-Pelletier & Cousineau (2018):

v = (df(df - 2)) \* ((2 \* (1 - r)) / n) \* (1 + g\^2 \* (n / (2 \* (1 - r)))) - (g\^2 / j\^2)

where n is the sample size of the experiment, df are the degrees of freedom (with df=2\*(n-1)), r is the correlation between the two dependent measures in the experiment, and j is the correction factor for small samples as described in Table 1.
The correlation r was not reported in any of the original articles.
We therefore always assumed a correlation of r = .50 to make our analysis comparable to standard (between-group) meta-analyses (Lakens, 2013) and because we were able to infer an average correlation of r $\approx$ .50 from a subsample of articles which provided sufficient information (Supplementary Methods 1).
A sensitivity analysis indicated that changing the assumed correlation to values from r = -.90 via r = .00 to r = .90 had no meaningful impact on the meta-analytic effect size (Supplementary Table 3).

For the meta-analysis of gender differences within each article, effect sizes were computed according to the following formulas in Table yy, and redundant groups were excluded from the analysis.

```{r, tab2, include=TRUE}
print("Nice table goes here")
```

Furthermore, the sampling variance was computed according to the following equation:

$sigma$between group = dfdf -2 2ñ1 + g2ñ2- g2J2

Where ñ is the harmonic mean of the group sizes and ñ = nmale nmalenfemale + nmale.
A sensitivity analysis reported in supplementary xxx highlights the stability of the effect size computed using the Bayesian meta analysis.

## Bayesian meta-analysis

We synthesized the effect sizes and their sampling variances using a Bayesian multilevel model.
This model had three levels, with infant participants nested in experiments and experiments nested in articles (Van den Noortgate et al., 2013).
We used a weakly-informative normal (N(0,1)) prior for the meta-analytic effect size and a weakly-informative half-Cauchy (HC(0, 0.3)) prior for all standard deviations (Williams et al., 2018).
A prior sensitivity analysis indicated that making these priors either more informative or less informative did not change the meta-analytic results (Supplementary Table 3).
All Bayesian models were fitted using the brms package (Version 2.16.3; Bürkner, 2017, 2018) in R (Version 4.1.2; R Core Team, 2021) and the Stan language (Version 2.29.2; Stan Development Team, 2022).
Markov Chain Monte Carlo (MCMC) sampling was used with four parallel chains, each sampling 10,000 draws (including 5,000 warm-up draws) from the posterior distribution.
To verify the convergence of the Markov chains, we examined rank plots as well as the R\_{hat} and N\_{eff} statistics (Vehtari et al., 2021; Supplementary Fig. 1).
For reporting, the credible interval (CrI) for each model parameter was computed as the 95% equal-tailed interval (ETI) of its posterior distribution, although replacing this with the 95% highest density interval (HDI) yielded highly similar results (Kruschke, 2015).

## Bayesian meta-regression

We examined the influence of three moderator variables on the mental rotation outcomes across studies, namely (a) the gender of the sample of infants, (b) the age of the sample of infants, and (c) the type of mental rotation task.
Gender was coded as a categorical predictor (mixed-gender sample, all-female sample, all-male sample) and contrast-coded using two successive difference contrasts so that we could compare all-female versus mixed samples and all-male samples versus all-female samples (Schad et al., 2020).
Age was coded as a continuous predictor in years and centered by subtracting the average across all studies.
Task was coded as a categorical predictor (habituation task, violation of expectation task) and contrast-coded using a scaled sum contrast (Schad et al., 2020).
We then included these predictors for gender, age, and task type as well as two predictors for the interaction between gender task type (i.e., [female - mixed] × age and [male - female] × age) into a Bayesian meta-regression model.
This model used the same random effects structure and sampling and sampling parameters as described above.

## Frequentist meta-analysis

We verified the results from our Bayesian analyses using classical (frequentist) meta-analysis and meta-regression.
We used the metafor package (Version 3.0.2; Viechtbauer, 2010) in R to specify the same three-level models as described above but dropping the Bayesian priors (Supplementary Table 3).
The models were fitted using restricted maximum likelihood estimation (REML).
To verify that REML converged on the correct estimates, we examined profile likelihood plots for the two variance components in the model (i.e., the between-experiment variance and the between-article variance; see Supplementary Fig. 2).

## Reporting bias and certainty assessment

We assessed publication bias by means of a funnel plot and Egger regression test (Egger et al. 1997).
We adapted code from the R package metaviz (Version 0.3.1; Kossmeier et al., 2020) to produce the funnel plot and used the R package metafor for the Egger regression test.
We examined heterogeneity in our study samples by means of a jackknife leave-one-out-analysis (Gee 2005).
We adapted code from the bootstrap R package (Version 2019.6; Scott Kostyshak et al., 2019) to fit the Bayesian model repeatedly to our study samples, leaving out one experiment sample for every iteration.
To take into account the variance inherent in Bayesian model estimation, we averaged over x iterations of the jackknife.

## Data & code availability

Blah blah blah

# References

<div id="refs"></div>

\newpage

# Supplementary information

```{r, tabs1, include=TRUE}
# Save within-analysis table of experiments
dat %>%
  mutate(
    age_mean = format_days_months(age_mean),
    age_sd = str_c(as.character(round(age_sd)), "d"),
  ) %>%
  transmute(
    Article = if_else(article == lag(article, default = ""), "", article),
    Experiment = group_long,
    `Sample size` = sample_size,
    `Females` = round(female_percent * sample_size),
    Age = str_c(age_mean, "±", age_sd, sep = " "),
    Task = task,
    `Stimulus type` = str_to_sentence(stimuli_presentation),
    `Stimulus dimensions` = stimuli_dimensions
  ) -> tabs1

# Save the table
write_csv(tabs1, file = here(tables_dir, "within_experiments.csv"), na = "n/a")

# Display the table
apa_table(
  tabs1,
  caption = "Experiments included in the main analysis", label = "S1",
  landscape = TRUE, font_size = "scriptsize"
)
```

```{r, figs1, include=TRUE, fig.height=10, fig.width=12, fig.cap="(ref:figs1-caption)"}
# Get posterior draws for the effect in each experiment
epred_draws_brm_gender <- epred_draws(res_brm_gender, dat_gender, ndraws = NULL) %>%
  mutate(experiment_f = factor(experiment))

# Create forest plot
dat_gender %>%
  # Make sure the plot will be ordered alphabetically by experiment IDs
  arrange(experiment) %>%
  mutate(
    experiment_f = fct_rev(fct_expand(factor(experiment), "model")),
    # Show article labels only for the first experiment per article
    article = if_else(article == lag(article, default = ""), "", article),
    # Compute frequentist confidence intervals
    ci_lb = gi - qnorm(0.05 / 2, lower.tail = FALSE) * sqrt(vi),
    ci_ub = gi + qnorm(0.05 / 2, lower.tail = FALSE) * sqrt(vi),
    ci_print = print_ci(gi, ci_lb, ci_ub)
  ) %>%
  # Prepare plotting canvas
  ggplot(aes(x = gi, y = experiment_f)) +
  geom_vline(xintercept = seq(-3, 3, 0.5), color = "grey90") +
  # Add article and group labels as text on the left
  geom_text(aes(x = -9.9, label = article), hjust = 0) +
  geom_text(aes(x = -7.1, label = group_long), hjust = 0) +
  # Add experiment-specific effect sizes and CIs as text on the right
  geom_text(aes(x = 3.7, label = print_num(gi)), hjust = 1) +
  geom_text(aes(x = 4.4, label = print_num(ci_lb)), hjust = 1) +
  geom_text(aes(x = 5.1, label = print_num(ci_ub)), hjust = 1) +
  # Add Bayesian highest posterior density intervals for each experiment
  stat_interval(
    aes(x = .epred),
    data = epred_draws_brm_gender,
    alpha = .8,
    point_interval = "mean_qi",
    .width = c(0.5, 0.95),
  ) +
  scale_color_grey(
    start = 0.85, end = 0.65,
    labels = as_mapper(~ scales::percent(as.numeric(.x)))
  ) +
  # Add experiment-specific effect sizes and CIs as dots with error bars
  geom_linerange(aes(xmin = ci_lb, xmax = ci_ub), size = 0.35) +
  geom_point(aes(size = ni), shape = 22, fill = "white") + # Or (1 / vi)?
  # Add posterior distribution for the meta-analytic effect
  stat_halfeye(
    aes(x = intercept, y = -1),
    draws_brm_gender,
    point_interval = "mean_qi",
    .width = c(0.95)
  ) +
  annotate(
    "text",
    x = c(-9.9, 3.7, 4.4, 5.1),
    y = -0.5,
    label = c(
      "Three-level model",
      print_num(summ_brm_gender$intercept),
      print_num(summ_brm_gender$intercept.lower),
      print_num(summ_brm_gender$intercept.upper)
    ),
    hjust = c(0, 1, 1, 1),
    fontface = "bold"
  ) +
  # Add headers for each of the for columns
  annotate( # Just to make the grid lines disappear
    "rect",
    xmin = -Inf,
    xmax = Inf,
    ymin = nrow(dat_gender) + 0.5,
    ymax = Inf,
    fill = "white"
  ) +
  annotate(
    "text",
    x = c(-9.9, -7.1, 0, 3.7, 4.4, 5.1),
    y = nrow(dat_gender) + 1.5,
    label = c(
      "Article",
      "Experiment",
      "Effect size plot",
      "Effect size",
      "Lower",
      "Upper"
    ),
    hjust = c(0, 0, 0.5, 1, 1, 1),
    fontface = "bold"
  ) +
  # Styling
  coord_cartesian(
    ylim = c(-1.5, nrow(dat_gender) + 2), expand = FALSE, clip = "off"
  ) +
  scale_x_continuous(breaks = seq(-3, 3, 0.5)) +
  annotate("segment", x = -3.3, xend = 3.3, y = -1.5, yend = -1.5) +
  labs(
    x = expression("Hedges'" ~ italic("g")),
    size = "Sample size",
    color = "CrI level"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(family = "Helvetica", color = "black"),
    axis.text.y = element_blank(),
    axis.ticks.x = element_line(colour = "black"),
    axis.title.x = element_text(hjust = 0.67, margin = margin(b = -15)),
    axis.title.y = element_blank(),
    panel.grid = element_blank(),
    text = element_text(family = "Helvetica")
  )

# Save the plot
ggsave(here(figures_dir, "forest_gender.pdf"), width = 12, height = 8)
```

(ref:figs1-caption) **Meta-analysis of gender differences.** A Bayesian three-level meta-analysis provided evidence for a small gender difference in mental rotation performance between male and female infants. White squares indicate the effect sizes (Hedges’ $g$) for gender difference in all individual experiments and black lines their 95% confidence intervals. Note that for experiments that observed a non-significant gender difference and did not specify the exact size of this effect, we assumed an effect size of $g = 0.00$. Gray bars indicate the 95% and 50% Bayesian credible interval (CrI) based on a Bayesian three-level random-effects model. Note that these intervals got shrunk towards the meta-analytic effect size because of partial pooling, that is, the three-level model regularizing the impact of experiments with small sample sizes and/or unrealistically large effect sizes. The last line shows the meta-analytic effect size from the three-level model (black dot) together with its 95% CrI (black line) and its posterior distribution (gray curve).

```{r, figs2, include=TRUE, fig.height=6, fig.width=12, fig.cap="(ref:figs2-caption)"}
# Extract convergence statistics from results
conv_stats <- with(summary(res_brm), tibble(
  parameter = c(
    "b_Intercept", "sd_article__Intercept", "sd_article:experiment__Intercept"
  ),
  name_expr = c(
    "Hedges ~ italic(g)", "italic(sigma)[article]", "italic(sigma)[experiment]"
  ),
  r_hat = print_num(c(fixed$Rhat, map_dbl(random, ~ .$Rhat)), digits = 4),
  bulk_ess = round(c(fixed$Bulk_ESS, map_dbl(random, ~ .$Bulk_ESS))),
  tail_ess = round(c(fixed$Tail_ESS, map_dbl(random, ~ .$Tail_ESS))),
  r_hat_expr = str_c("italic(R)[hat] == ", r_hat),
  bulk_ess_expr = str_c("italic(N)[eff(bulk)] == ", bulk_ess),
  tail_ess_expr = str_c("italic(N)[eff(tail)] == ", tail_ess),
  y_pos = c(0.7, 0.75, 0.8)
))

# Create trace plot
n_draws <- n_iter - n_warmup
bayesplot::mcmc_trace(
  res_brm,
  pars = conv_stats$parameter, facet_args = list(ncol = 1)
) +
  geom_text(
    aes(y = 0.45, label = name_expr, color = NA),
    conv_stats,
    y = 0.45, x = -800, color = "black", parse = TRUE, angle = 90
  ) +
  geom_text(
    aes(y = y_pos, label = r_hat_expr, color = NA),
    conv_stats,
    x = 13000, hjust = 1, color = "black", parse = TRUE,
  ) +
  geom_text(
    aes(y = y_pos, label = bulk_ess_expr, color = NA),
    conv_stats,
    x = 16000, hjust = 1, color = "black", parse = TRUE,
  ) +
  geom_text(
    aes(y = y_pos, label = tail_ess_expr, color = NA),
    conv_stats,
    x = 19000, hjust = 1, color = "black", parse = TRUE,
  ) +
  labs(x = "Sample number", y = NULL, color = "MCMC\nchain") +
  coord_cartesian(expand = FALSE, clip = "off") +
  scale_x_continuous(limits = c(0, n_draws), breaks = seq(0, n_draws, 2000)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_color_grey(start = 0.0, end = 0.9) +
  theme_classic() +
  theme(
    axis.line = element_line(colour = "black"),
    axis.text = element_text(family = "Helvetica", color = "black"),
    axis.ticks = element_line(color = "black"),
    plot.margin = margin(t = 10, l = 25),
    strip.background = element_blank(),
    strip.text = element_blank(),
    text = element_text(family = "Helvetica", color = "black")
  )

# Save the plot
ggsave(here(figures_dir, "trace.pdf"), width = 12, height = 6)
```

(ref:figs2-caption) Convergence checks.

```{r, frequentist_results}
# Three-level model
res_ml <- rma.mv(
  gi, vi,
  random = ~ 1 | article / experiment,
  data = dat,
  slab = experiment
)
print(res_ml)
# forest(res_ml)

# Check share of variance at each level of the model
round(res_ml$sigma2[1] / sum(res_ml$sigma2), 3) # Between-article (ICC)
round(res_ml$sigma2[2] / sum(res_ml$sigma2), 3) # Within-article (between-exp.)

# # Profile likelihodd plots for checking that REML has converged
# profile(res_ml)

# Meta-regression with gender, age, and task
res_reg <- rma.mv(
  gi, vi,
  mods = ~ gender * age_c + task,
  random = ~ 1 | article / experiment,
  data = dat,
  slab = experiment
)
print(res_reg)
# forest(res_full)

# Extract standard deviations and ICCs
vars_ml <- list(matrix())

# Export results as a table
map2_dfr(
  c("Meta-analysis", "Meta-regression"),
  list(res_ml, res_reg),
  function(name, res) {
    # Intercept / regression weights
    tibble(
      Model = c(name, rep(NA, length(res$b) - 1)),
      Parameter = rownames(res$b),
      Estimate = print_num(as.numeric(res$b)),
      SE = print_num(res$se),
      z = print_num(res$zval),
      p = print_num(res$pval, digits = 3),
      ci_lower = print_num(res$ci.lb),
      ci_upper = print_num(res$ci.ub)
    ) %>%
      # Re-format confidence interval
      mutate(
        `95% CI` = str_c("[", ci_lower, ", ", ci_upper, "]"),
        .keep = "unused"
      ) %>%
      # Add variances
      bind_rows(tibble(
        Parameter = res$s.name,
        Estimate = print_num(res$sigma2)
      ))
  }
) %>%
  mutate(
    Parameter = c(
      "Hedges' g",
      "s2article",
      "s2experiment",
      "Intercept",
      "Female - mixed",
      "Male - female",
      "Age (per year)",
      "Habituation - VoE",
      "(Female - mixed) × age",
      "(Male - female) × age",
      "s2article",
      "s2experiment"
    )
  ) -> tab_freq

# Save the table
write_csv(tab_freq, file = here(tables_dir, "frequentist_results.csv"), na = "")
```

```{r, sensitivity_analysis, eval=FALSE, include=TRUE}
# Compute empirical estimate of the correlation between dependent samples
dat %>%
  mutate(
    # Get effect size from paired samples t-test, one sample t-test, or ANOVA
    d_diff = case_when(
      !is.na(d) ~ d,
      !is.na(d_z_t) ~ d_z_t,
      !is.na(d_z_f) ~ d_z_f,
      !is.na(d_z_diff) ~ d_z_diff
    ),
    # Compute empirical correlation
    # Basesd on the SD of the difference and the SDs within the two conditions
    sd_diff = mean_diff / d_diff,
    ri = (sd_diff^2 - sd_novel^2 - sd_familiar^2) /
      (-2 * sd_novel * sd_familiar)
  ) %>%
  pull(ri) -> ri_empirical

# Summarise the estimated correlations
summary(ri_empirical)

# Re-run the meta-analysis for different values of r_assumed
seq(-0.9, 0.9, by = 0.3) %>%
  set_names(., str_c("r = ", print_num(.))) %>%
  map(function(ri_sens) {

    # Re-compute standard errors of the effect sizes
    mutate(
      dat,
      vi = (dfi / (dfi - 2)) * ((2 * (1 - ri_sens)) / ni) *
        (1 + gi^2 * (ni / (2 * (1 - ri_sens)))) - (gi^2 / ji^2),
      sei = sqrt(vi)
    ) -> dat_ri_sens

    # Re-run meta-analysis
    update(res_brm, newdata = dat_ri_sens, refresh = 0)
  }) -> res_ri_sens

# Prior sensitivity analysis
list(
  "Intercept U(-10,10)" = c(
    set_prior("uniform(-10, 10)", class = "Intercept"),
    set_prior("cauchy(0, 0.3)", class = "sd")
  ),
  "Intercept N(0,0.2)" = c(
    set_prior("normal(0, 0.2)", class = "Intercept"),
    set_prior("cauchy(0, 0.3)", class = "sd")
  ),
  "SD U(0,10)" = c(
    set_prior("normal(0, 1)", class = "Intercept"),
    set_prior("uniform(0, 10)", class = "sd")
  ),
  "SD Student-t(10,0,0.2)" = c(
    set_prior("normal(0, 1)", class = "Intercept"),
    set_prior("student_t(10, 0, 0.2)", class = "sd")
  )
) %>%
  # Re-run the meta-analysis with different priors
  map(
    function(prior_sens) update(res_brm, prior = prior_sens, refresh = 0)
  ) -> res_prior_sens

# Create a table summarizing all sensitivity analyses
map2_dfr(
  c(res_ri_sens, res_prior_sens),
  c(names(res_ri_sens), names(res_prior_sens)),
  function(res, name) {

    # Compute ICC
    var_experiment <- as.matrix(
      res,
      variable = "sd_article:experiment__Intercept"
    )^2
    var_article <- as.matrix(
      res,
      variable = "sd_article__Intercept"
    )^2
    var_total <- var_experiment + var_article
    icc <- var_article / var_total
    icc_row <- mean_qi(icc) %>%
      rename(Estimate = y, `l-95% CI` = ymin, `u-95% CI` = ymax)

    # Create a data frame row of summary statistics
    with(summary(res), bind_rows(fixed, bind_rows(random))) %>%
      bind_rows(icc_row) %>%
      transmute(
        parameter = c("Hedges' g", "SD_{article}", "SD_{experiment}", "ICC"),
        Estimate = print_ci(Estimate, `l-95% CI`, `u-95% CI`)
      ) %>%
      pivot_wider(names_from = parameter, values_from = Estimate) %>%
      bind_cols(Simulation = name, .)
  }
) -> tab_sens

# Save the table
write_csv(tab_sens, file = here(tables_dir, "sensitivity_analysis.csv"))
```
